<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CS180 Project 4: Image Mosaicing</title>
  <style>
    body {
      font-family: "Segoe UI", Arial, sans-serif;
      line-height: 1.6;
      margin: 0;
      padding: 0;
      background: #fafafa;
      color: #222;
    }
    header {
      background: #004080;
      color: white;
      padding: 2rem 1rem;
      text-align: center;
    }
    main {
      max-width: 900px;
      margin: 2rem auto;
      background: white;
      padding: 2rem;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
    }
    section {
      margin-bottom: 3rem;
    }
    h2 {
      border-bottom: 2px solid #004080;
      padding-bottom: 0.5rem;
      color: #004080;
    }
    img {
      display: block;
      max-width: 100%;
      margin: 1rem auto;
      border-radius: 8px;
    }
    .caption {
      text-align: center;
      font-size: 0.9rem;
      color: #555;
    }
    footer {
      text-align: center;
      padding: 1rem;
      background: #f0f0f0;
      color: #666;
    }
    code {
      background: #f5f5f5;
      padding: 2px 4px;
      border-radius: 3px;
      font-size: 0.95em;
    }
    .mosaic-row {
    display: flex;
    justify-content: center;
    align-items: center;
    gap: 10px; /* 图片间距，可调 */
    margin-bottom: 10px;
  }
  
  .mosaic-row img {
    width: 30%; /* 每张图宽度，可根据实际调整 */
    height: auto;
    border: 1px solid #ccc;
  }
  </style>
</head>
<body>
  <header>
    <h1>Image Mosaicing</h1>
    <p>Project 4 — Homography, Warping, and Blending</p>
  </header>

  <main>
    <section id="A1">
      <h2>A.1 Shoot and Digitize Pictures</h2>
      <p>
        I captured two sets of photographs by rotating my camera around a fixed center of projection. 
        Each pair of images has a 40–60% overlap for reliable feature matching.
      </p>
      <img src="images/A.2.1.jpg" alt="Source Image 1">
      <img src="images/A.2.2.jpg" alt="Source Image 2">
      <img src="images/test4.1.jpg" alt="Source Image 3">
      <img src="images/test4.2.jpg" alt="Source Image 4">
      <p class="caption">Example pairs of images with projective transformation.</p>
    </section>

    <section id="A2">
  <h2>A.2 Recover Homographies</h2>

  <p>
    To align two images under a projective transformation, we must estimate the
    <strong>homography matrix</strong> <code>H</code> such that <code>p' = H·p</code>.
    The transformation is computed from pairs of corresponding points 
    <code>(x, y) → (x', y')</code> between the two images.
  </p>

  <h3>Point Correspondences</h3>
  <p>
    I manually selected more than four point correspondences using 
    the website tool given. The correspondences are visualized below.
  </p>

  <img src="images/correspondences.png" alt="Point correspondences visualization">
  <p class="caption">Corresponding points visualized between the two images.</p>

  <h3>System of Equations</h3>
  <p>
    For each correspondence, two linear equations are generated to form the system 
    <code>Ah = b</code>, where <code>h</code> contains the 8 unknown homography parameters.
  </p>
  <pre>
For each (x, y) → (x', y'):
  [-x, -y, -1,  0,  0,  0, x*x', y*y'] * h = -x'
  [ 0,  0,  0, -x, -y, -1, x*y', y*y'] * h = -y'
  </pre>

  <p>
    This gives us a system <code>A·h = b</code> with 2n equations for n correspondences.
    When n &gt; 4, the system is overdetermined and solved using least squares:
  </p >

  <pre>h = (AᵀA)⁻¹Aᵀb</pre>

  <p>
    The recovered homography matrix <code>H</code> is reshaped as:
  </p >

  <pre>
H = [[h11 h12 h13]
     [h21 h22 h23]
     [h31 h32   1]]
  </pre>

  <p>
    The final computed homography for my image pair is:
  </p >
  <h3>Recovered Homography Matrix</h3>
  <pre>
H =
|  1.4248282     2.6777553e-02   -1280.97236 |
|  2.5172202e-01 1.25429927      -452.174673 |
|  1.3775899e-04 -2.4165614e-08   1.00000000 |

  </pre>
  <p class="caption">Computed homography mapping Image 2 → Image 1.</p>

  <p>
    The resulting homography accurately aligns planar regions between the two images,
    enabling seamless warping and blending in later stages.
  </p>
</section>


<section id="A3">
  <h2>A.3 Warp the Images</h2>
  <p>
    I implemented inverse warping with two interpolation methods:
  </p>
  <ul>
    <li><strong>Nearest Neighbor:</strong> Fast but blocky results.</li>
    <li><strong>Bilinear:</strong> Smoother and visually pleasing, but slower.</li>
  </ul>

  <h3>Python Implementation</h3>
<pre>
Algorithm: Warp Image Using Homography H

Input:
  - Image im
  - Homography matrix H

Steps:
  1. Compute inverse homography H⁻¹.
  2. Determine output bounds (min_x, max_x, min_y, max_y).
  3. For each pixel (x_out, y_out) in output image:
       a. Map back to input coordinates (x_w, y_w, 1) = H⁻¹ · (x_out, y_out, 1).
       b. Normalize: x_w = x_w / w,  y_w = y_w / w.

     ▸ Nearest Neighbor:
         - Round (x_w, y_w) to nearest integer.
         - Assign pixel value from nearest location.

     ▸ Bilinear Interpolation:
         - Find floor(x_w), floor(y_w) → (x₀, y₀).
         - Compute fractional parts dx, dy.
         - Interpolate using 4 neighboring pixels:
           (1−dx)(1−dy)*I[y₀,x₀] + dx(1−dy)*I[y₀,x₀+1]
           + (1−dx)dy*I[y₀+1,x₀] + dxdy*I[y₀+1,x₀+1]

  4. Store result in warped image.

Output:
  - Warped image aligned by H.
</pre>


  <h3>Warped Images</h3>
  <img src="images/warp_nn.png" alt="Warped image using Nearest Neighbor">
  <p class="caption">Warped image using Nearest Neighbor interpolation.</p>

  <img src="images/warp.png" alt="Warped image comparison">
  <p class="caption">Warped image comparison.</p>

  <h3>Rectification Example</h3>
  <p>
    Below are 2 rectified images where a tilted view was transformed to appear fronto-parallel.
  </p>
  <img src="images/rectified.png" alt="Rectified result 1">
  <img src="images/rectified1.png" alt="Rectified result 2">
  <img src="images/rectified2.png" alt="Rectified result 3">
  <p class="caption">Rectification using manually selected correspondences.</p>
</section>


<section id="A4">
  <h2>A.4 Blend the Images into a Mosaic</h2>
  <p>
    I warped the second image into the coordinate frame of the first (reference) image using the computed homography. 
    Each mosaic was created from a pair of input images, aligned within a shared mosaic canvas determined by their transformed corners.
  </p>

  <p>
    To blend overlapping regions, I used <strong>alpha-weighted averaging with a linear falloff mask</strong> near the image boundaries. 
    This smooth transition reduces visible seams and creates a more natural mosaic. 
    Since each mosaic consists of two images, simple weighted blending was sufficient and no multi-band blending was required.
  </p>

  <p>
    To reduce edge artifacts where two warped images overlap, I generated an <em>alpha mask</em> for each image. 
    Pixel weights were higher at the image center and gradually decreased toward the edges, forming a smooth falloff. 
    This ensures that in overlapping areas, both images contribute proportionally instead of one abruptly overwriting the other.
    The blended pixel value at each position <code>(x, y)</code> was computed as:
  </p>

  <pre><code>
mosaic(x, y) = (α₁ * I₁(x, y) + α₂ * I₂(x, y)) / (α₁ + α₂)
  </code></pre>

  <p>
    This weighted averaging allows for smooth transitions between images, significantly reducing sharp seams or exposure jumps 
    along image boundaries. Although minor ghosting may occur when alignment is imperfect, the overall result appears visually 
    coherent and continuous.
  </p>

<!-- ========== Mosaic 1 ========== -->
<h3>Mosaic Example 1</h3>
<div class="mosaic-row">
  <img src="images/A.2.1.jpg" alt="Input image 1 for mosaic 1">
  <img src="images/A.2.2.jpg" alt="Input image 2 for mosaic 1">
  <img src="images/mosaic_result.jpg" alt="Mosaic 1 result">
</div>
<p class="caption">Resulting mosaic from image pair 1.</p >

<!-- ========== Mosaic 2 ========== -->
<h3>Mosaic Example 2</h3>
<div class="mosaic-row">
  <img src="images/test2.1.jpg" alt="Input image 1 for mosaic 2">
  <img src="images/test2.2.jpg" alt="Input image 2 for mosaic 2">
  <img src="images/mosaic_result1.jpg" alt="Mosaic 2 result">
</div>
<p class="caption">Resulting mosaic from image pair 2.</p >

<!-- ========== Mosaic 3 ========== -->
<h3>Mosaic Example 3</h3>
<div class="mosaic-row">
  <img src="images/test3.1.jpg" alt="Input image 1 for mosaic 3">
  <img src="images/test3.2.jpg" alt="Input image 2 for mosaic 3">
  <img src="images/mosaic_result3.jpg" alt="Mosaic 3 result">
</div>
<p class="caption">Resulting mosaic from image pair 3.</p >

<!-- ========== Mosaic 4 ========== -->
<h3>Mosaic Example 4</h3>
<div class="mosaic-row">
  <img src="images/test4.1.jpg" alt="Input image 1 for mosaic 4">
  <img src="images/test4.2.jpg" alt="Input image 2 for mosaic 4">
  <img src="images/mosaic_result4.jpg" alt="Mosaic 4 result">
</div>
<p class="caption">Resulting mosaic from image pair 4.</p >

</section>

<!-- =============================== -->
<!-- ===       Part B Section     === -->
<!-- =============================== -->

<section id="B1">
  <h2>B.1: Harris Corner Detection and ANMS</h2>
  <p>
    To locate distinctive and repeatable feature points, I first applied 
    the <strong>Harris corner detector</strong> on each image to identify regions 
    with strong local gradients in both directions. This step highlights areas 
    such as corners or texture-rich points that are likely to be good matches across views.
  </p>
  <p>
    Since the Harris detector often produces many nearby detections in textured regions, 
    I then used ANMS to select 
    a spatially uniform subset of corners. ANMS retains points with high corner strength 
    while ensuring an even spatial distribution, which improves matching stability later on.
  </p>

  <img src="images/B1_harris_corners.png" alt="Harris corners detected">
  <p class="caption">Detected Harris corners (red) overlaid on the grayscale image.</p>

  <img src="images/B1_anms_corners.png" alt="ANMS corners selected">
  <p class="caption">Corners after Adaptive Non-Maximal Suppression — uniformly distributed across the scene.</p>
</section>

<section id="B2">
  <h2>B.2: Feature Descriptor Extraction</h2>
  <p>
    Around each selected corner, I extracted a <strong>40×40 pixel patch</strong> 
    from the grayscale image and resized it to an <strong>8×8 descriptor</strong> 
    after applying bias and gain normalization:
  </p>
  <pre><code>normalized = (patch - mean) / std</code></pre>
  <p>
    This normalization removes global lighting effects and contrast differences, 
    making the descriptors more comparable across different images. 
    Each descriptor is flattened into a 64-dimensional vector used for matching.
  </p>

  <img src="images/descriptors_patches.png" alt="40x40 patches">
    <img src="images/descriptors_patches1.png" alt="40x40 patches">
  <p class="caption">Example 40×40 patches around detected corners and their corresponding 8×8 normalized descriptors.</p>
</section>

<section id="B3">
  <h2>B.3: Feature Matching</h2>
  <p>
    With the 8×8 descriptors from both images, I performed feature matching 
    using the <strong>Lowe ratio test</strong>. For each descriptor in the first image, 
    the two nearest descriptors in the second image were found based on Euclidean distance. 
    A match was accepted if the ratio <code>d₁ / d₂ &lt; 0.75</code>.
  </p>

  <p>
    The ratio test helps remove ambiguous correspondences and reduces false matches. 
    A threshold of <strong>0.75</strong> provided a good balance — 
    allowing enough matches for RANSAC while maintaining accuracy. 
    The visualization below shows several examples of matched features.
  </p>

  <img src="images/matches_pairs.png" alt="Feature matching pairs">
  <p class="caption">Matched feature pairs between two images, visualized with connecting lines.</p>

  <img src="images/matches_pairs1.png" alt="Feature matching pairs">
  <p class="caption">Additional example of feature matching results on a different image pair.</p>
</section>

<section id="B4">
  <h2>B.4: RANSAC for Robust Homography Estimation</h2>
  <p>
    Using the feature correspondences, I implemented a <strong>4-point RANSAC</strong> algorithm
    to estimate a robust homography.  
    Each iteration randomly selects 4 matches, computes a candidate homography,
    and counts inliers based on reprojection error.
  </p>

  <pre><code>
for iteration in range(num_iter):
    sample = random 4 correspondences
    H = compute_homography(sample)
    inliers = matches[reprojection_error(H) < tol]
    keep best H with max inliers
  </code></pre>

  <p>
    The final homography was refined with all inliers, producing a stable and accurate transformation.
  </p>

  <!-- Automatic Mosaics -->
  <h3>Automatic Mosaic Examples</h3>

  <div class="mosaic-row">
    <img src="images/A.2.1.jpg" alt="Input A1">
    <img src="images/A.2.2.jpg" alt="Input A2">
    <img src="images/b4_mosaic_1.jpg" alt="Automatic Mosaic 1">
  </div>
  <p class="caption">Automatic mosaic from A.2 pair using RANSAC.</p>

  <div class="mosaic-row">
    <img src="images/test3.1.jpg" alt="Input T3.1">
    <img src="images/test3.2.jpg" alt="Input T3.2">
    <img src="images/b4_mosaic_2.jpg" alt="Automatic Mosaic 2">
  </div>
  <p class="caption">Automatic mosaic from test3 pair.</p>

  <div class="mosaic-row">
    <img src="images/test4.1.jpg" alt="Input T4.1">
    <img src="images/test4.2.jpg" alt="Input T4.2">
    <img src="images/b4_mosaic_3.jpg" alt="Automatic Mosaic 3">
  </div>
  <p class="caption">Automatic mosaic from test4 pair.</p>

  <h3>Automatic Mosaic Examples and Comparison</h3>

  <!-- ========== Mosaic Pair 1 ========== -->
  <h4>Example 1: A.2 Pair</h4>
  <div class="mosaic-row">
    <img src="images/A.2.1.jpg" alt="Input A.2.1">
    <img src="images/A.2.2.jpg" alt="Input A.2.2">
  </div>
  <p class="caption">Input image pair used for Example 1.</p>

  <div class="mosaic-row">
    <img src="images/mosaic_result.jpg" alt="Manual Mosaic 1">
    <img src="images/b4_mosaic_1.jpg" alt="Automatic Mosaic 1">
  </div>
  <p class="caption">Left: Manual mosaic (Part A). Right: Automatic mosaic (Part B).</p>

  <hr>

  <!-- ========== Mosaic Pair 2 ========== -->
  <h4>Example 2: test3 Pair</h4>
  <div class="mosaic-row">
    <img src="images/test3.1.jpg" alt="Input test3.1">
    <img src="images/test3.2.jpg" alt="Input test3.2">
  </div>
  <p class="caption">Input image pair used for Example 2.</p>

  <div class="mosaic-row">
    <img src="images/mosaic_result3.jpg" alt="Manual Mosaic 2">
    <img src="images/b4_mosaic_2.jpg" alt="Automatic Mosaic 2">
  </div>
  <p class="caption">Left: Manual mosaic (Part A). Right: Automatic mosaic (Part B).</p>

  <hr>

  <!-- ========== Mosaic Pair 3 ========== -->
  <h4>Example 3: test4 Pair</h4>
  <div class="mosaic-row">
    <img src="images/test4.1.jpg" alt="Input test4.1">
    <img src="images/test4.2.jpg" alt="Input test4.2">
  </div>
  <p class="caption">Input image pair used for Example 3.</p>

  <div class="mosaic-row">
    <img src="images/mosaic_result4.jpg" alt="Manual Mosaic 3">
    <img src="images/b4_mosaic_3.jpg" alt="Automatic Mosaic 3">
  </div>
  <p class="caption">Left: Manual mosaic (Part A). Right: Automatic mosaic (Part B).</p>

  <hr>

  <p>
    Across all three examples, the <strong>automatic mosaics (right)</strong> are closely aligned
    with the manually stitched results (left).  
    Minor parallax artifacts appear in regions with large depth variation, but the planar
    areas are well aligned.  
    This demonstrates that the end-to-end pipeline — including Harris corner detection,
    ANMS selection, feature descriptor extraction, feature matching, and 4-point RANSAC —
    successfully produces robust homographies for image mosaicing.
  </p>

</section>

  </main>

</body>
</html>
